
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Deep Learning Assignment 3: Conditional Deep Convolutional Generative Adversarial Networks (cDCGAN)}
    \author{Huey Ning Lok}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle


\section{Abstract}\label{abstract}

For this assignment, I decided to build a cDCGAN to
generate images based on their class labels. The conditional GAN was
first suggested by Mirza and Osindero (2014), and embeds class labels,
\(y\), of a given dataset, \(x\), into both the discriminator and
generator of the GAN to enable the creation of targeted images. The
generated image results showed high in-class diversity but low
inter-class diversity among both models, with the inter-class diversity
being lower in the novel dataset model vs CIFAR10 model. The 
inter-class mode collapse is hypothesized to be a result of both the model's architectural weaknesses and the nature of the underlying dataset. Potential solutions for minimizing mode collapse are suggested for future work.

\section{Introduction}\label{introduction}

According to Mirza and Osindero (2014), "Generative adversarial nets can
be extended to a conditional model if both the generator and
discriminator are conditioned on some extra information \(y\). \(y\)
could be any kind of auxiliary information, such as class labels or data
from other modalities. We can perform the conditioning by feeding \(y\)
into both the discriminator and generator as additional input layer."\\

Some motivations for including the class label within the input are
(Brownlee, 2019):

\begin{itemize}
\item
  To improve GAN performance by making use of additional information
  (class labels) that is correlated with the input images.
\item
  To generate targeted images.
\end{itemize}

\section{Encoding Class Labels}\label{encoding-class-labels}

In a vanilla conditional GAN network consisting of only linear layers, \(y\) could be
embedded into the input via simple concatenation. According to Brownlee
(2019), one of the best practices in encoding and incorporating class
labels into both the discriminator and generator models entails "using
an embedding layer followed by a fully connected layer with a linear
activation that scales the embedding to the size of the image before
concatenating it in the model as an additional channel or feature map".

In a conditional DCGAN network with convolutional layers, the encoding
of class labels becomes a bit trickier since the images are not
flattened into a single dimension before being fed into the network.
Instead, the DCGAN network normally takes in an input of 3 dimensions,
i.e. width x heights x number of channels. The image is then
continuously convolved through multiple kernels in the network without
ever having to be flattened until a final classification decision has to
be made using a linear layer (e.g. sigmoid) at the output.

\section{Main References}\label{main-references}

To build my cDCGAN model, I made heavy reference of the following
sources:

\begin{itemize}
\item
  \textbf{Pytorch DCGAN Tutorial (Inkawhich, 2017).} The tutorial walked
  through the concept and implementation of the DCGAN model in Pytorch.
\item
  \textbf{Conditional GAN Code in Pytorch (Linder-Noren, 2019).} This
  Github repo covers the implementation of a regular conditional GAN
  model. Linder-Noren made use of Pytorch's \texttt{nn.Embedding} layer
  to embed the class labels in the model. He concatenated the embedding
  layer with the flattened input before feeding it into the GAN network.
  Since I was using a conditional DCGAN, I could only borrow his idea of
  using a \texttt{nn.Embedding} layer but still needed to figure out an
  appropriate location in the architecture where I could embed my class
  labels.
\item
  \textbf{Conditional DCGAN Tutorial in Keras (Desai, 2018).} This
  Medium post walked through the implementation of a conditional DCGAN
  in Keras. Desai noted that "It wasn't really intuitive ... how the
  conditioning input can be applied to the Convolutional layers of the
  Discriminator", but ultimately justified concatenating the labels
  right before the fully connected layers (near the end of the network)
  on the basis that "the Discriminator is learning high level features
  from the image and using them in conjunction with the conditioning
  input to make the final decision". This was the method I used to embed
  my class labels in my cDCGAN model.
\item
  \textbf{cGAN and cDCGAN on MNIST and Celeb Faces in Pytorch (Kang,
  2017).} This Github repo implements both a conditional GAN and
  conditional DCGAN in Pytorch to generate MNIST digits and celebrity
  faces. In contrast to Desai (2018), Kang (2017) embedded the class
  labels to the input very early on in the discriminator by first
  feeding the image and class labels seperately into the same
  convolutional layer, so that both the image and class labels have the
  same output shape after the first convolution. After that, the image
  and class labels are concatenated together and fed through the rest of
  the network. The figure below illustrates Kang's methodology:
\end{itemize}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{kang-cdcgan.png}
    \end{center}



\section{Architectural Choices}\label{architectural-choices}

\subsection{Conditional
Discriminator}\label{conditional-discriminator}

My biggest challenge in building the conditional discriminator was
deciding how to embed the class labels into the network. Linder-Noren
(2019) simply concatenated the class labels with the input before
feeding it into the network, but he could do so because he was using a
regular conditional GAN without convolutional layers. Convolutional
layers take in multidimensional inputs, and so the concatenation of
class labels is not as straightforward as if the input was
1-dimensional.\\

The two main ways of embedding class labels that I considered were:

\begin{itemize}
\item
  Kang's (2017) method of embedding the class labels to the image after
  feeding both through a convolutional layer. This means the image was
  "conditioned" on the class before going through the main convolutional
  layers.
\item
  Desai's (2018) method of embedding the class labels to the image after
  the convolutional layers, but before the input is fed into the linear
  layers section of the network. That is, given a conventional deep CNN
  model structured as:
  
  \texttt{INPUT\ -\textgreater{}\ {[}CONV\ -\textgreater{}\ RELU\ -\textgreater{}\ POOL{]}*2\ -\textgreater{}\ FC\ -\textgreater{}\ RELU\ -\textgreater{}\ FC}
  (Karpathy \& Johnson, 2017),
  
   the class labels will be concatenated
  after the \texttt{CONV} block but before the \texttt{FC} layers.
\end{itemize}

I decided to follow Desai's (2018) method as I liked his reasoning that
the class label should be considered a "higher-level" feature that would
be embedded near the tail-end of the cDCGAN to provide
further information to the network.

\textbf{Convolutional Layers in Discriminator} - The convolutional
layers of my discriminator were based on the Pytorch DCGAN tutorial
(Inkawhich, 2017), which was in turn, based on the original DCGAN paper
(Radford, Metz, \& Chintala, 2015).\\

\textbf{Embedding the Class Label} - The output from the final
convolutional layer was flattened and concatenated with the class label
layer to create a "conditioned" layer of dimension: {[}batch size,
number of classes + 1{]}. For example, with a batch size of 128 and 3
class labels, the convolutional layer output is flattened to {[}128,
1{]}, the class labels are represented as an embedded layer of {[}128,
3{]}, and so the concatenated layer is {[}128, 3+1{]} = {[}128, 4{]}.\\

\textbf{Linear Layers in Discriminator} The concatenated layer was then
fed into a \texttt{FC\ -\textgreater{}\ LeakyRELU} block right before
the final sigmoid layer. This was mostly just following Desai's (2018)
architecture, with minor tweaking on my end to adjust the number of
input and output features. I used 512 output features for the linear
layer because that was the number used by both Desai (2018) and
Linder-Noren (2017), though frankly I am uncertain as to whether there
is a strong reason for choosing 512 as the number of neurons (other than
the fact that 512 is a power of 2).\\

Since Desai's (2018) code was written in Keras, I followed his choice of
concatenation placement, but referenced Linder-Noren's (2017)
implementation of Pytorch's \texttt{nn.Embedding} layer to embed the
class labels.

\subsection{Conditional Generator}\label{conditional-generator}

The conditional generator was relatively simpler to implement. I
followed Desai's (2018) method of first concatenating the class labels
with the input noise to produce a 1D input, before reshaping the 1D
input into a 3D input with dimensions {[}latent vector size + number of
class labels, 1, 1{]}. This 3D input was then fed into the convolutional
layers to generate a fake image.

\subsection{Complete Conditional
Model}\label{complete-conditional-model}

The complete model is similar to the DCGAN model, except that an
additional class label input is now fed into both the discriminator and
the generator to be embedded and used within the network.\\

Aside from adding class labels as an additional parameter, an additional
step was to generate fake labels for generating fake images and training
the discriminator on the fake images:\\

\texttt{generated\_labels\ =\ torch.randint(self.n\_classes,\ (b\_size,)).type(torch.LongTensor)}\\

where \texttt{n\_classes} is the number of classes, and \texttt{b\_size}
is the batch size. The output is a 1D tensor of size
\texttt{{[}b\_size{]}} that contains a random uniform distribution of
each class label (since number of classes is the class label ceiling).

\newpage
\section{Data}\label{data}

Two datasets were used to test the efficacy of the cDCGAN: the CIFAR10
dataset and a novel dataset.\\

The CIFAR10 dataset consisted of 10 classes: `airplane', `automobile',
`bird', `cat', `deer', `dog', `frog', `horse', `ship', `truck'. 50,000
images were used to train the cDCGAN, with an even distribution of 5,000
images per class.\\

The novel dataset consisted of 3 classes: cartoon, cyberpunk, and noir
digital artwork. The digital art was scraped from
\url{https://www.artstation.com/}. The classes have 9924, 9859, and 3567
entries respectively.

\section{Parameter Values}\label{parameter-values}

Most parameters were set according to the Pytorch tutorial on building a
DCGAN model. The following custom adjustments were also carried out:

\begin{itemize}
\item
  The images were resized to (32, 32, 3) for faster computation, and the
  feature map size for both the generator and the discriminator were
  also set to 32.
\item
  The batch size was set to 128.
\end{itemize}

\section{Implementation}\label{implementation}

The cDCGAN was trained for 60 epochs on the CIFAR10 dataset and 60
epochs on the novel dataset. The parameters were kept the same across
both training procedures, aside from the dataloaders.

\subsection{Training Process}\label{training-process}

The training process was carried out as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Initiailize cDCGAN object with designated parameters.
\item
  Train for a set number of epochs. The model automatically saves
  checkpoints at every 5 epochs, as well as after the last epoch.
\item
  If more training is desired, load model from last epoch to continue
  training.
\end{enumerate}

\newpage
\section{Results and Analysis}\label{results-and-analysis}

Note that in order to run the cells containing the
\texttt{plot\_gen\_images()} and \texttt{plot\_progression()} functions
to visualize the results, the cells below in the \textbf{Code} section
have to be run first to create the model objects, load checkpoints (if
applicable), and define the functions. I just shifted the plotting functions up
here so that the results can be seen before the analysis.

\subsection{CIFAR10 Model Results}\label{cifar10-model-results}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}293}]:} \PY{n}{plot\PYZus{}gen\PYZus{}images}\PY{p}{(}\PY{n}{cifar\PYZus{}classes}\PY{p}{,} \PY{n}{cdcgan\PYZus{}cifar}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_0.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_1.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_2.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_3.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_4.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_5.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_6.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_7.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_8.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_1_9.png}
    \end{center}

    
    \subsection{Generator Progression for "Dog"
Class}\label{generator-progression-for-dog-class}

I decided to only show the progression for a single class since there is
low inter-class diversity, so the progression does not differ
significantly between classes anyway.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}271}]:} \PY{n}{plot\PYZus{}progression}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN\PYZhy{}cifar/fake\PYZus{}images/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fig\PYZus{}w} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{,} \PY{n}{fig\PYZus{}h} \PY{o}{=} \PY{l+m+mi}{40}\PY{p}{,} \PY{n}{nrows} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}
          \PY{n}{ncols} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
There are 50 images to be plotted. Adjust params accordingly.

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={1.2\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_3_1.png}
    \end{center}

    \subsection{CIFAR10 Model Analysis}\label{cifar10-model-analysis}

The cDCGAN model trained on the CIFAR10 dataset is able to generate a
decent variety of fake images within a single class, which suggests that
the within-class mode is not collapsing. Regarding the inter-class
variation, we can see some visual differences across the different class
images, but it is definitely not obvious enough that one could
differentiate the classes from each other unless through careful
scrutiny and cherry-picking.\\

It would appear that my model's generator has learned to create a
certain type of fake image that can be considered valid for a high
variety of classes, i.e. it is defaulting to a mode in the
image-validity distribution, and is collapsing to that mode.\\

The mode collapse is visualized as generated images that look similar
across the different classes. For example, most of the fake images look
similar, despite being generated for different classes. The "frog" and
"ship" classes do have some visual characteristics that allow me to pick
them out if I squint very hard, but it's all rather hacky. The "ship"
images all come with a "wavy" blur, perhaps due to the presence of water
in real ship images. The "frog" images tend to have a semi-curved bottom
border filled with white - at first I thought this was because of the
frog's belly, but when I analyzed the real images I saw that most frog
pictures featured a frog against a white backdrop (unlike the other
images which almost always had a filled natural background). I think
this reinforces the idea that the underlying data quality can have a
huge impact on the cDCGAN performance.\\

While my generated images look a far stretch from the real images, and
honestly look very similar across classes, the fact that \emph{some} of
the classes appear to have developed \emph{some} distinctive (albeit
unexpected) visual features that allow them to be distinguished from the
other classes suggests that my cDCGAN model is capable of creating
targeted images given sufficient inter-class variation within the
underlying data or with further improvement to the model to tackle mode
collapse.\\

I also suspect the possibility that the underlying dataset (CIFAR10) may
simply have small inter-class variation between certain classes in
particular, and so we see mode collapse across these classes, but
successful targeted generation across classes that inherently had highly
differentiated visual features. In his report on GANs, Goodfellow (2016)
stated that Minibatch GANs are able to create rather distinguishable
targeted images when trained on the CIFAR10 dataset, though to be honest
his generated images still look rather similar and undistinguishable to
me. The figure below shows Goodfellow's (2016) Minibatch GAN results on CIFAR10:

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{goodfellow-cifar10.png}
    \end{center}

\newpage
\subsection{Novel Dataset Model
Results}\label{novel-dataset-model-results}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}272}]:} \PY{n}{plot\PYZus{}gen\PYZus{}images}\PY{p}{(}\PY{n}{dataset}\PY{o}{.}\PY{n}{classes}\PY{p}{,} \PY{n}{cdcgan}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_5_0.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_5_1.png}
    \end{center}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_5_2.png}
    \end{center}

    
    \subsection{Generator Progression for "Cyberpunk"
Class}\label{generator-progression-for-cyberpunk-class}

I started testing out my cDCGAN on the novel dataset, so the image
progression plot differs slightly from how I presented it for the
CIFAR10 dataset. Specifically:

\begin{itemize}
\tightlist
\item
  I generated only 10 images per epoch instead of 64, and so a row of
  images is created vs a full grid.\\
\item
  I changed the latent noise vector \(z\) around the 10th epoch of
  training, so the generated images for epochs 1, 3, 6, and 9 are
  rendered irrelevant in visualizing the progression of the generator.
  As such, I have not included them in the plot below. The first row of
  images below starts from epoch 11, and the last row marks epoch 60.
\end{itemize}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}284}]:} \PY{n}{plot\PYZus{}progression}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN/fake\PYZus{}images/}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cyberpunk}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{fig\PYZus{}w} \PY{o}{=} \PY{l+m+mi}{40}\PY{p}{,} \PY{n}{fig\PYZus{}h} \PY{o}{=} \PY{l+m+mi}{30}\PY{p}{,} \PY{n}{nrows} \PY{o}{=} \PY{l+m+mi}{19}\PY{p}{,}
          \PY{n}{ncols} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_7_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Novel Dataset Model
Analysis}\label{novel-dataset-model-analysis}

For the cDCGAN model trained on my novel dataset, we see that the batch
of generated images for any given class looks sufficiently varied, but a
comparison of the generated images across different classes bears little
to no visual difference. The difference is even harder to detect for the
novel dataset model vs the CIFAR10 model, but this could be because the
data is inherently less varied (there is a high possibility of high
overlap between cyberpunk, cartoon, and noir type artwork). That being
said, if you squint really hard, there are actually some slight color
variation differences.\\

On hindsight, the imbalance in the class label numbers may also have
contributed to mode collapse in the model, but I did not want to balance
the classes since that would result in cutting roughly 60\% of the
cartoon and cyberpunk classes. Since I was building a GAN, I figured
that the class imbalance was acceptable. However, the embedding of class
labels in the input may have caused the class imbalance to negatively
affect the model's performance.

\newpage
\section{Mode Collapse}\label{mode-collapse}

Goodfellow (2016) defined mode collapse as "a problem that occurs when
the generator learns to map several different input \(z\) values to the
same output point." Real world data usually consists of multimodal
distributions, e.g. there are usually several common features that we
expect to see in an image belonging to a given class. If the generator
is somehow able to identify one of the distribution modes and the
discriminator has not been trained well, it will fail to recognize that
the generated data is simply coming from a single mode (Desai, 2018).
Within the cDCGAN, this is represented as low inter-class variation
since the generator can fall back on common modes among different
classes, e.g. the color blue (if most class images contain a sky), high
contrast colors (this can be seen in my novel dataset, where most
artists like to draw high-contrast images regardless of the genre).

\section{Future Work}\label{future-work}

There are a few suggested solutions for reducing mode collapse in GANs.
I have listed some here to be potentially explored in future work, but
unfortunately did not implement these solutions in this paper due to
time constraints.

\begin{itemize}
\item
  \textbf{Minibatch Discrimination (Salimans et al., 2016 as cited in
  Goodfellow, 2016)}. In minibatch discrimination, the generator is
  penalized for generating similar looking samples. Real images and
  generated images are fed into the discriminator separately in
  different batches; the similarity of the image x with images in the
  same batch is then computed as a similarity term, \(o(x)\). \(o(x)\)
  is then concatenated in one of the dense layers in the discriminator
  to classify whether this image is real or generated. If the mode
  starts to collapse, the similarity of generated images increases. The
  discriminator can then use the similarity score \(o(x)\) to detect
  generated images and penalize the generator if mode is collapsing.
\item

  \textbf{Unrolled GANs (Metz et al., 2016 as cited in Goodfellow,
  2016)}. As explained by Goodfellow (2016): "The idea of unrolled GANs
  is to build a computational graph describing \(k\) steps of learning
  in the discriminator, then backpropagate through all \(k\) of these
  steps of learning when computing the gradient on the generator. Fully
  maximizing the value function for the discriminator takes tens of
  thousands of steps, but Metz et al. (2015) found that unrolling for even small numbers of steps, like 10 or
  fewer, can noticeably reduce the mode dropping problem." The
  \(k\)-step lookahead discourages the generator from exploiting a local
  optima that could easily be counteracted by the discriminator, and so
  unrolled GANs reduce the probability of the generator overfitting to a
  specific discriminator (Hui, 2018).
\end{itemize}

\newpage
    \section{Code}\label{code}
    
Python Notebook implementations of the code can be viewed at the following urls:

\begin{itemize}
\item
\url{https://github.com/hueyning/DeepLearningTutorial/blob/master/DL3-cDCGAN/conditional-DCGAN.ipynb}

\item
\url{https://nbviewer.jupyter.org/github/hueyning/DeepLearningTutorial/blob/master/DL3-cDCGAN/conditional-DCGAN.ipynb}
\end{itemize}

    \subsection*{Import Libraries}\label{import-libraries}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}240}]:} \PY{k+kn}{from} \PY{n+nn}{\PYZus{}\PYZus{}future\PYZus{}\PYZus{}} \PY{k}{import} \PY{n}{print\PYZus{}function}
          \PY{c+c1}{\PYZsh{}\PYZpc{}matplotlib inline}
          \PY{k+kn}{import} \PY{n+nn}{argparse}
          \PY{k+kn}{import} \PY{n+nn}{os}
          \PY{k+kn}{import} \PY{n+nn}{random}
          \PY{k+kn}{import} \PY{n+nn}{torch}
          \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
          \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{parallel}
          \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{backends}\PY{n+nn}{.}\PY{n+nn}{cudnn} \PY{k}{as} \PY{n+nn}{cudnn}
          \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
          \PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data}
          \PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{as} \PY{n+nn}{dset}
          \PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{transforms} \PY{k}{as} \PY{n+nn}{transforms}
          \PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{as} \PY{n+nn}{vutils}
          \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{image} \PY{k}{as} \PY{n+nn}{mpimg}
          \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{gridspec} \PY{k}{as} \PY{n+nn}{gridspec}
          \PY{k+kn}{from} \PY{n+nn}{torchsummary} \PY{k}{import} \PY{n}{summary}
          \PY{k+kn}{from} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{utils} \PY{k}{import} \PY{n}{save\PYZus{}image}
          \PY{k+kn}{import} \PY{n+nn}{torchvision}
          
          \PY{c+c1}{\PYZsh{} Set random seed for reproducibility}
          \PY{n}{manualSeed} \PY{o}{=} \PY{l+m+mi}{999}
          \PY{c+c1}{\PYZsh{}manualSeed = random.randint(1, 10000) \PYZsh{} use if you want new results}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Random Seed: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{manualSeed}\PY{p}{)}
          \PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{manualSeed}\PY{p}{)}
          \PY{n}{torch}\PY{o}{.}\PY{n}{manual\PYZus{}seed}\PY{p}{(}\PY{n}{manualSeed}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Random Seed:  999

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}240}]:} <torch.\_C.Generator at 0x118b4c150>
\end{Verbatim}
            
    \subsection*{Set Parameter Values}\label{set-parameter-values}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Root directory for dataset}
        \PY{n}{dataroot} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{images}\PY{l+s+s1}{\PYZsq{}}
        
        \PY{c+c1}{\PYZsh{} Number of workers for dataloader}
        \PY{n}{workers} \PY{o}{=} \PY{l+m+mi}{2}
        
        \PY{c+c1}{\PYZsh{} Batch size during training}
        \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{128}
        
        \PY{c+c1}{\PYZsh{} Spatial size of training images. All images will be resized to this size using a}
        \PY{n}{transformer}\PY{o}{.}
        \PY{n}{image\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{32}
        
        \PY{c+c1}{\PYZsh{} Number of channels in the training images. For color images this is 3}
        \PY{n}{nc} \PY{o}{=} \PY{l+m+mi}{3}
        
        \PY{c+c1}{\PYZsh{} Size of z latent vector (i.e. size of generator input)}
        \PY{n}{nz} \PY{o}{=} \PY{l+m+mi}{100}
        
        \PY{c+c1}{\PYZsh{} Size of feature maps in generator}
        \PY{n}{ngf} \PY{o}{=} \PY{l+m+mi}{32}
        
        \PY{c+c1}{\PYZsh{} Size of feature maps in discriminator}
        \PY{n}{ndf} \PY{o}{=} \PY{l+m+mi}{32}
        
        \PY{c+c1}{\PYZsh{} Learning rate for optimizers}
        \PY{n}{lr} \PY{o}{=} \PY{l+m+mf}{0.0002}
        
        \PY{c+c1}{\PYZsh{} Beta1 hyperparam for Adam optimizers}
        \PY{n}{beta1} \PY{o}{=} \PY{l+m+mf}{0.5}
        
        \PY{c+c1}{\PYZsh{} Number of GPUs available. Use 0 for CPU mode.}
        \PY{n}{ngpu} \PY{o}{=} \PY{l+m+mi}{1}
        
        \PY{c+c1}{\PYZsh{} Decide which device we want to run on}
        \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda:0}\PY{l+s+s2}{\PYZdq{}} \PY{k}{if} \PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)} \PY{o+ow}{and} \PY{n}{ngpu} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)} \PY{k}{else} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cpu}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}

    \subsection*{Import Data}\label{import-data}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}images}\PY{p}{(}\PY{n}{dataloader}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{image\PYZus{}number} \PY{o}{=} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{model} \PY{o}{=} \PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Function to plot a sample of images from the dataloader, alongside their class}
        \PY{l+s+sd}{labels.}
        \PY{l+s+sd}{    If a model is assigned to the model parameter, the predicted labels will be printed}
        \PY{l+s+sd}{as well.}
        
        \PY{l+s+sd}{    Input:}
        \PY{l+s+sd}{        dataloader (DATALOADER)}
        \PY{l+s+sd}{            Dataloader of dataset.}
        
        \PY{l+s+sd}{        classes (ARR)}
        \PY{l+s+sd}{            Array type object containing the class labels (strings) in the order that}
        \PY{l+s+sd}{            corresponds with the numerical key in the dataloader.}
        
        \PY{l+s+sd}{        image\PYZus{}number (INT)}
        \PY{l+s+sd}{            Number of images to plot from the dataloader. image\PYZus{}number should not exceed batch size.}
        \PY{l+s+sd}{            Since images are plotted in a row, any number \PYZgt{} 10 could cause display issues.}
        \PY{l+s+sd}{            Default: 8.}
        
        \PY{l+s+sd}{        model (PYTORCH MODEL)}
        \PY{l+s+sd}{            Optional parameter. If a model is provided, the predicted labels from the}
        \PY{l+s+sd}{            model for each of the images will be printed as well.}
        \PY{l+s+sd}{            Default: None.}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
        
            \PY{c+c1}{\PYZsh{} get images and true labels}
            \PY{n}{images}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{dataloader}\PY{p}{)}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} plot images}
            \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,}\PY{l+m+mi}{16}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{vutils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{images}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{n}{image\PYZus{}number}\PY{p}{]}\PY{p}{,}
        \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} print true labels}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True labels: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{     }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}5s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{classes}\PY{p}{[}\PY{n}{labels}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{]} \PY{k}{for} \PY{n}{j} \PY{o+ow}{in}
        \PY{n+nb}{range}\PY{p}{(}\PY{n}{image\PYZus{}number}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
            \PY{k}{if} \PY{n}{model}\PY{p}{:}
                \PY{c+c1}{\PYZsh{} predict image classes using custom net}
                \PY{n}{outputs} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{images}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} the outputs are energies for the 10 classes.}
                \PY{c+c1}{\PYZsh{} the higher the energy for a class, the more the network thinks that the image}
        \PY{o+ow}{is} \PY{n}{of} \PY{n}{the} \PY{n}{particular} \PY{n}{class}\PY{o}{.}
                \PY{c+c1}{\PYZsh{} So, we get the index of the highest energy:}
                \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{predicted} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{outputs}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                \PY{c+c1}{\PYZsh{} print predicted labels}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted:  }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{   }\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}5s}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{classes}\PY{p}{[}\PY{n}{predicted}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{]} \PY{k}{for} \PY{n}{j} \PY{o+ow}{in}
        \PY{n+nb}{range}\PY{p}{(}\PY{n}{image\PYZus{}number}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        
        
        \PY{k}{def} \PY{n+nf}{get\PYZus{}target\PYZus{}index}\PY{p}{(}\PY{n}{dataset}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Given a dataset, this function returns a dictionary of classes, where the value of each class}
        \PY{l+s+sd}{    is a dictionary containing the class indices and the number of datapoints in the class}
        
        \PY{l+s+sd}{    Input:}
        \PY{l+s+sd}{        dataset (IMAGEFOLDER)}
        \PY{l+s+sd}{            Dataset should be ImageFolder class.}
        
        \PY{l+s+sd}{    Output:}
        \PY{l+s+sd}{        idx\PYZus{}dct (DCT)}
        \PY{l+s+sd}{            Nested dictionary with the class name as key, and a dictionary containing the}
        \PY{l+s+sd}{            \PYZsq{}indices\PYZsq{} and \PYZsq{}length\PYZsq{} of the class as values.}
        \PY{l+s+sd}{            Example format:}
        \PY{l+s+sd}{            idx\PYZus{}dct = \PYZob{} \PYZsq{}class\PYZus{}A\PYZsq{}:\PYZob{}}
        \PY{l+s+sd}{                        \PYZsq{}indices\PYZsq{}: [1,2,3,4,5],}
        \PY{l+s+sd}{                        \PYZsq{}length\PYZsq{}: 5}
        \PY{l+s+sd}{                        \PYZcb{},}
        \PY{l+s+sd}{                        \PYZsq{}class\PYZus{}B\PYZsq{}:\PYZob{}}
        \PY{l+s+sd}{                        \PYZsq{}indices\PYZsq{}: [6,7,8],}
        \PY{l+s+sd}{                        \PYZsq{}length\PYZsq{}: 3}
        \PY{l+s+sd}{                        \PYZcb{},}
        \PY{l+s+sd}{                        \PYZsq{}class\PYZus{}C\PYZsq{}:\PYZob{}}
        \PY{l+s+sd}{                        \PYZsq{}indices\PYZsq{}: [100,101,102,103],}
        \PY{l+s+sd}{                        \PYZsq{}length\PYZsq{}: 4}
        \PY{l+s+sd}{                        \PYZcb{}\PYZcb{}}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{targets} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{t}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{t} \PY{o+ow}{in} \PY{n}{dataset}\PY{o}{.}\PY{n}{samples}\PY{p}{]}\PY{p}{)}
            \PY{n}{idx\PYZus{}dct} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
        
            \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n}{dataset}\PY{o}{.}\PY{n}{class\PYZus{}to\PYZus{}idx}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                \PY{n}{idx\PYZus{}dct}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indices}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{(}\PY{n}{targets} \PY{o}{==} \PY{n}{v}\PY{p}{)}\PY{o}{.}\PY{n}{nonzero}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}
                \PY{n}{idx\PYZus{}dct}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{idx\PYZus{}dct}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{indices}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{idx\PYZus{}dct}
        
        
        \PY{k}{def} \PY{n+nf}{plot\PYZus{}batch}\PY{p}{(}\PY{n}{dataloader}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
        \PY{l+s+sd}{    Plot images from a dataloader}
        \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
            \PY{n}{real\PYZus{}batch} \PY{o}{=} \PY{n+nb}{next}\PY{p}{(}\PY{n+nb}{iter}\PY{p}{(}\PY{n}{dataloader}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{off}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{vutils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{real\PYZus{}batch}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{64}\PY{p}{]}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,}
        \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \subsection*{CIFAR10 Dataset}\label{cifar10-dataset}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{} transform images to tensor and normalize}
         \PY{n}{transform} \PY{o}{=} \PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}
             \PY{p}{[}\PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
              \PY{n}{transforms}\PY{o}{.}\PY{n}{Normalize}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} create dataloaders}
         \PY{n}{trainset} \PY{o}{=} \PY{n}{torchvision}\PY{o}{.}\PY{n}{datasets}\PY{o}{.}\PY{n}{CIFAR10}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                                                 \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{)}
         \PY{n}{trainloader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{trainset}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{n}{batch\PYZus{}size}\PY{p}{,}
                                                   \PY{n}{shuffle}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{num\PYZus{}workers}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{n}{cifar\PYZus{}classes} \PY{o}{=} \PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{plane}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{car}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bird}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cat}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                    \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{deer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{frog}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{horse}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ship}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{truck}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} plot sample of images}
         \PY{n}{plot\PYZus{}images}\PY{p}{(}\PY{n}{trainloader}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Files already downloaded and verified
True labels:   bird      ship      deer       dog      deer       cat     horse      bird

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_17_1.png}
    \end{center}
    
    \subsection*{Novel Dataset}\label{novel-dataset}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}4}]:} \PY{c+c1}{\PYZsh{} Create the dataset}
        \PY{n}{dataset} \PY{o}{=} \PY{n}{dset}\PY{o}{.}\PY{n}{ImageFolder}\PY{p}{(}\PY{n}{root}\PY{o}{=}\PY{n}{dataroot}\PY{p}{,}
                                   \PY{n}{transform}\PY{o}{=}\PY{n}{transforms}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                                   \PY{n}{transforms}\PY{o}{.}\PY{n}{Resize}\PY{p}{(}\PY{n}{image\PYZus{}size}\PY{p}{)}\PY{p}{,}
                                   \PY{n}{transforms}\PY{o}{.}\PY{n}{CenterCrop}\PY{p}{(}\PY{n}{image\PYZus{}size}\PY{p}{)}\PY{p}{,}
                                   \PY{n}{transforms}\PY{o}{.}\PY{n}{RandomHorizontalFlip}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
                                   \PY{n}{transforms}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                                   \PY{n}{transforms}\PY{o}{.}\PY{n}{Normalize}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                                   \PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{target\PYZus{}idx\PYZus{}dct} \PY{o}{=} \PY{n}{get\PYZus{}target\PYZus{}index}\PY{p}{(}\PY{n}{dataset}\PY{p}{)}
        \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n}{target\PYZus{}idx\PYZus{}dct}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Class }\PY{l+s+si}{\PYZob{}k\PYZcb{}}\PY{l+s+s2}{ has }\PY{l+s+si}{\PYZob{}v[\PYZsq{}length\PYZsq{}]\PYZcb{}}\PY{l+s+s2}{ entries.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{n}{dataloader} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{utils}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{DataLoader}\PY{p}{(}\PY{n}{dataset}\PY{p}{,} \PY{n}{batch\PYZus{}size} \PY{o}{=} \PY{n}{batch\PYZus{}size}\PY{p}{,} \PY{n}{shuffle} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        
        \PY{n}{plot\PYZus{}images}\PY{p}{(}\PY{n}{dataloader}\PY{p}{,} \PY{n}{dataset}\PY{o}{.}\PY{n}{classes}\PY{p}{,} \PY{n}{image\PYZus{}number} \PY{o}{=} \PY{l+m+mi}{8}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
Class cartoon has 9924 entries.
Class cyberpunk has 9859 entries.
Class noir has 3567 entries.
True labels:  cyberpunk     cyberpunk     cartoon     cartoon     cartoon     cartoon        noir     cartoon

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{conditional-DCGAN_files/conditional-DCGAN_19_1.png}
    \end{center}
    
    \newpage
    \subsection*{cDCGAN Model}\label{cdcgan-model}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}74}]:} \PY{k}{def} \PY{n+nf}{weights\PYZus{}init}\PY{p}{(}\PY{n}{m}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    Custom weights initialization called on netG and netD}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{classname} \PY{o}{=} \PY{n}{m}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}class\PYZus{}\PYZus{}}\PY{o}{.}\PY{n+nv+vm}{\PYZus{}\PYZus{}name\PYZus{}\PYZus{}}
         
             \PY{k}{if} \PY{n}{classname}\PY{o}{.}\PY{n}{find}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Conv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{normal\PYZus{}}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{,} \PY{l+m+mf}{0.02}\PY{p}{)}
         
             \PY{k}{elif} \PY{n}{classname}\PY{o}{.}\PY{n}{find}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BatchNorm}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{normal\PYZus{}}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{weight}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{0.02}\PY{p}{)}
                 \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{constant\PYZus{}}\PY{p}{(}\PY{n}{m}\PY{o}{.}\PY{n}{bias}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}97}]:} \PY{k}{class} \PY{n+nc}{cDiscriminator}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
         
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{p}{:}
         
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{cDiscriminator}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{ngpu} \PY{o}{=} \PY{n}{ngpu}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{label\PYZus{}embedding} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{n\PYZus{}classes}\PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{convolution\PYZus{}layers} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
                     \PY{c+c1}{\PYZsh{} input is (nc) x 64 x 64}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{nc}\PY{p}{,} \PY{n}{ndf}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ndf) x 32 x 32}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{ndf}\PY{p}{,} \PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ndf*2) x 16 x 16}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ndf*4) x 8 x 8}
                     \PY{c+c1}{\PYZsh{} nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),}
                     \PY{c+c1}{\PYZsh{} nn.BatchNorm2d(ndf * 8),}
                     \PY{c+c1}{\PYZsh{} nn.LeakyReLU(0.2, inplace=True),}
         
                     \PY{c+c1}{\PYZsh{} state size. (ndf*8) x 4 x 4}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{ndf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                 \PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linear\PYZus{}layers} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
         
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{in\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{1} \PY{o}{+} \PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{c+c1}{\PYZsh{} flattened output from last conv + embedding}
                               \PY{n}{out\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{512}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} arbitrary + based on external references}
         
                     \PY{n}{nn}\PY{o}{.}\PY{n}{LeakyReLU}\PY{p}{(}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{inplace}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{p}{,}
         
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{in\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{512}\PY{p}{,} \PY{c+c1}{\PYZsh{} output from last linear layer}
                               \PY{n}{out\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} true or false image}
         
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Sigmoid}\PY{p}{(}\PY{p}{)}
                 \PY{p}{)}
         
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n+nb}{input}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
         
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{convolution\PYZus{}layers}\PY{p}{(}\PY{n+nb}{input}\PY{p}{)} \PY{c+c1}{\PYZsh{} run input through convolutional layers}
                 \PY{c+c1}{\PYZsh{} print(x.shape) \PYZsh{} output shape: (128,1,1,1)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} flatten output from main}
                 \PY{c+c1}{\PYZsh{} print(x.shape) \PYZsh{} output shape: (128,1)}
                 \PY{n}{y} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{label\PYZus{}embedding}\PY{p}{(}\PY{n}{labels}\PY{p}{)} \PY{c+c1}{\PYZsh{} create label layer}
                 \PY{c+c1}{\PYZsh{} print(y.shape) \PYZsh{} output shape: (128,3)}
                 \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} concatenate flattened output to label layer}
                 \PY{c+c1}{\PYZsh{} print(x.shape) \PYZsh{} output shape: (128,4)}
                 \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linear\PYZus{}layers}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{c+c1}{\PYZsh{} run flattened + merged layer through linear layers}
         
                 \PY{k}{return} \PY{n}{x}
\end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}98}]:} \PY{c+c1}{\PYZsh{} Generator Code}
         
         \PY{k}{class} \PY{n+nc}{cGenerator}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
         
             \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{p}{:}
         
                 \PY{n+nb}{super}\PY{p}{(}\PY{n}{cGenerator}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{ngpu} \PY{o}{=} \PY{n}{ngpu}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{n}{n\PYZus{}classes}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{label\PYZus{}emb} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{n\PYZus{}classes}\PY{p}{)}
         
                 \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{main} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
         
                     \PY{c+c1}{\PYZsh{} input is Z + n\PYZus{}classes, going into a convolution}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{nz} \PY{o}{+} \PY{n}{n\PYZus{}classes}\PY{p}{,} \PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ngf*8) x 4 x 4}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(}\PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ngf*4) x 8 x 8}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(} \PY{n}{ngf} \PY{o}{*} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{ngf}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{ngf}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{k+kc}{True}\PY{p}{)}\PY{p}{,}
         
                     \PY{c+c1}{\PYZsh{} state size. (ngf*2) x 16 x 16}
                     \PY{c+c1}{\PYZsh{} nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),}
                     \PY{c+c1}{\PYZsh{} nn.BatchNorm2d(ngf),}
                     \PY{c+c1}{\PYZsh{} nn.ReLU(True),}
         
                     \PY{c+c1}{\PYZsh{} state size. (ngf) x 32 x 32}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{ConvTranspose2d}\PY{p}{(} \PY{n}{ngf}\PY{p}{,} \PY{n}{nc}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{bias}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{,}
                     \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}
                     \PY{c+c1}{\PYZsh{} state size. (nc) x 64 x 64}
                 \PY{p}{)}
         
             \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n+nb}{input}\PY{p}{,} \PY{n}{labels}\PY{p}{)}\PY{p}{:}
         
                 \PY{c+c1}{\PYZsh{} Concatenate label embedding and noise to produce input}
                 \PY{n}{flat\PYZus{}embed\PYZus{}input} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{label\PYZus{}emb}\PY{p}{(}\PY{n}{labels}\PY{p}{)}\PY{p}{,} \PY{n+nb}{input}\PY{p}{)}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} reshape flattened layer to torch.Size([128, nz+n\PYZus{}classes, 1, 1])}
                 \PY{n}{reshaped\PYZus{}input} \PY{o}{=} \PY{n}{flat\PYZus{}embed\PYZus{}input}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{nz} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         
                 \PY{n}{gen\PYZus{}img} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{main}\PY{p}{(}\PY{n}{reshaped\PYZus{}input}\PY{p}{)}
         
                 \PY{k}{return} \PY{n}{gen\PYZus{}img}
\end{Verbatim}

\newpage
   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}143}]:} \PY{k}{class} \PY{n+nc}{cDCGAN}\PY{p}{(}\PY{n+nb}{object}\PY{p}{)}\PY{p}{:}
          
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Conditional DCGAN class.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
          
              \PY{k}{def} \PY{n+nf}{\PYZus{}checkDirectory}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dirName}\PY{p}{)}\PY{p}{:}
          
                  \PY{k}{if} \PY{o+ow}{not} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{exists}\PY{p}{(}\PY{n}{dirName}\PY{p}{)}\PY{p}{:}
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}dirName\PYZcb{}}\PY{l+s+s2}{ directory does not exist. Making }\PY{l+s+si}{\PYZob{}dirName\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                      \PY{n}{os}\PY{o}{.}\PY{n}{makedirs}\PY{p}{(}\PY{n}{dirName}\PY{p}{)}
          
                  \PY{k}{else}\PY{p}{:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}dirName\PYZcb{}}\PY{l+s+s2}{ directory exists.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          
              \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{dataloader}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{save\PYZus{}dir}\PY{p}{,} \PY{n}{num\PYZus{}epochs}\PY{p}{,}
                           \PY{n}{criterion}\PY{p}{,} \PY{n}{netD}\PY{p}{,} \PY{n}{netG}\PY{p}{,} \PY{n}{optimizerD}\PY{p}{,} \PY{n}{optimizerG}\PY{p}{,} \PY{n}{device}\PY{p}{)}\PY{p}{:}
          
                  \PY{c+c1}{\PYZsh{} data parameters}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dataloader} \PY{o}{=} \PY{n}{dataloader}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classes} \PY{o}{=} \PY{n}{classes} \PY{c+c1}{\PYZsh{} class labels}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)} \PY{c+c1}{\PYZsh{} number of classes}
          
                  \PY{c+c1}{\PYZsh{} save file locations}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}checkDirectory}\PY{p}{(}\PY{n}{save\PYZus{}dir}\PY{p}{)} \PY{c+c1}{\PYZsh{} check whether save dir exists}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{checkpoint\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{save\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{checkpoints}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}checkDirectory}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{checkpoint\PYZus{}dir}\PY{p}{)} \PY{c+c1}{\PYZsh{} create checkpoints dir}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fake\PYZus{}image\PYZus{}dir} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{save\PYZus{}dir}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{fake\PYZus{}images}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}checkDirectory}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fake\PYZus{}image\PYZus{}dir}\PY{p}{)} \PY{c+c1}{\PYZsh{} create fake images dir}
          
                  \PY{c+c1}{\PYZsh{} model parameters}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs} \PY{o}{=} \PY{n}{num\PYZus{}epochs} \PY{c+c1}{\PYZsh{} number of epochs to train for}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{start\PYZus{}epoch} \PY{o}{=} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} the starting epoch}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion} \PY{o}{=} \PY{n}{criterion} \PY{c+c1}{\PYZsh{} loss function}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{real\PYZus{}label} \PY{o}{=} \PY{l+m+mi}{1} \PY{c+c1}{\PYZsh{} Establish convention for real and fake labels during training.}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fake\PYZus{}label} \PY{o}{=} \PY{l+m+mi}{0}
          
                  \PY{c+c1}{\PYZsh{} networks init}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD} \PY{o}{=} \PY{n}{netD}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG} \PY{o}{=} \PY{n}{netG}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerD} \PY{o}{=} \PY{n}{optimizerD}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerG} \PY{o}{=} \PY{n}{optimizerG}
          
                  \PY{c+c1}{\PYZsh{} device}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{device} \PY{o}{=} \PY{n}{device} \PY{c+c1}{\PYZsh{} specify device being used}
          
                  \PY{c+c1}{\PYZsh{} Create fixed noise to visualize the progression of the generator}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fixed\PYZus{}noise} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{nz}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{device}\PY{p}{)} \PY{c+c1}{\PYZsh{} torch.Size([64,100])}
          
          
              \PY{k}{def} \PY{n+nf}{generate\PYZus{}fake\PYZus{}images}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{class\PYZus{}index\PYZus{}tensor}\PY{p}{,} \PY{n}{noise}\PY{p}{,} \PY{n}{image\PYZus{}name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{random}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
          \PY{n}{save} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
          
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        Generate a batch of fake images using current generator weights.}
          
          \PY{l+s+sd}{        Inputs}
          
          \PY{l+s+sd}{            class\PYZus{}index\PYZus{}tensor (LongTensor)}
          \PY{l+s+sd}{                The class index to create fake images for. The number of fake images}
          \PY{l+s+sd}{	        generated is equal to the length of the tensor.}
          \PY{l+s+sd}{                So a tensor filled with 10 \PYZdq{}1\PYZdq{}s will generate 10 images for}
          \PY{l+s+sd}{                the class that corresponds to \PYZdq{}1\PYZdq{}.}
          
          \PY{l+s+sd}{            noise (Tensor)}
          \PY{l+s+sd}{                Random noise that will be put through the generator weights to produce an image.}
          
          \PY{l+s+sd}{            image\PYZus{}name (STR)}
          \PY{l+s+sd}{                Image name for the saved file.}
          \PY{l+s+sd}{                If running this function in model training, image\PYZus{}name should contain a changing variable,}
          \PY{l+s+sd}{                otherwise the files will just keep overwriting each other with the same name.}
          \PY{l+s+sd}{                Default: \PYZsq{}random\PYZsq{} (in case save = True but no image\PYZus{}name provided)}
          
          \PY{l+s+sd}{            save (BOOL)}
          \PY{l+s+sd}{                If save is TRUE, the image file will be saved in the specified self.fake_image_dir}
          \PY{l+s+sd}{                Otherwise, just return the image data for plotting.}
          \PY{l+s+sd}{                Default: TRUE}
          
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
                  \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{} create fake images for a the labels in class\PYZus{}index\PYZus{}tensor}
                      \PY{n}{fake} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG}\PY{p}{(}\PY{n}{noise}\PY{p}{,} \PY{n}{class\PYZus{}index\PYZus{}tensor}\PY{p}{)}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{cpu}\PY{p}{(}\PY{p}{)}
          
                  \PY{k}{if} \PY{n}{save}\PY{p}{:} \PY{c+c1}{\PYZsh{} save images in the fake\PYZus{}image\PYZus{}dir}
                      \PY{n}{save\PYZus{}image}\PY{p}{(}\PY{n}{fake}\PY{o}{.}\PY{n}{data}\PY{p}{,} \PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}self.fake\PYZus{}image\PYZus{}dir\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+si}{\PYZob{}image\PYZus{}name\PYZcb{}}\PY{l+s+s1}{.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                                 \PY{n}{nrow}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
          
                  \PY{k}{return} \PY{n}{fake}\PY{o}{.}\PY{n}{data}
          
          
              \PY{k}{def} \PY{n+nf}{train}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
          
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        Training loop}
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
                  \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No epochs set for training. Exiting training loop.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                      \PY{k}{return}
          
                  \PY{c+c1}{\PYZsh{} Lists to keep track of progress}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} generator loss}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{D\PYZus{}losses} \PY{o}{=} \PY{p}{[}\PY{p}{]} \PY{c+c1}{\PYZsh{} discriminator loss}
                  \PY{n}{iters} \PY{o}{=} \PY{l+m+mi}{0}
          
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Starting Training Loop...}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} For each epoch}
                  \PY{k}{for} \PY{n}{epoch} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{start\PYZus{}epoch}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{start\PYZus{}epoch} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs}\PY{p}{)}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{} For each batch in the dataloader}
                      \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{p}{(}\PY{n}{imgs}\PY{p}{,} \PY{n}{class\PYZus{}labels}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dataloader}\PY{p}{)}\PY{p}{:}
          
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
                          \PY{c+c1}{\PYZsh{} (1) Update D network: maximize log(D(x)) + log(1 \PYZhy{} D(G(z)))}
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
          
                          \PY{c+c1}{\PYZsh{}\PYZsh{} Train with all\PYZhy{}real batch}
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Format batch}
                          \PY{n}{real\PYZus{}imgs} \PY{o}{=} \PY{n}{imgs}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{device}\PY{p}{)}
                          \PY{n}{b\PYZus{}size} \PY{o}{=} \PY{n}{real\PYZus{}imgs}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Set ground truth labels as REAL}
                          \PY{n}{validity\PYZus{}label} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{full}\PY{p}{(}\PY{p}{(}\PY{n}{b\PYZus{}size}\PY{p}{,}\PY{p}{)}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{real\PYZus{}label}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Forward pass real batch through D}
                          \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{p}{(}\PY{n}{real\PYZus{}imgs}\PY{p}{,} \PY{n}{class\PYZus{}labels}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate loss on all\PYZhy{}real batch}
                          \PY{n}{errD\PYZus{}real} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{validity\PYZus{}label}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate gradients for D in backward pass}
                          \PY{n}{errD\PYZus{}real}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                          \PY{n}{D\PYZus{}x} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
          
          
                          \PY{c+c1}{\PYZsh{}\PYZsh{} Train with all\PYZhy{}fake batch}
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
                          \PY{c+c1}{\PYZsh{} Generate batch of latent vectors}
                          \PY{n}{noise} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{b\PYZus{}size}\PY{p}{,} \PY{n}{nz}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)} \PY{c+c1}{\PYZsh{} torch.Size([128, 10])}
          
                          \PY{c+c1}{\PYZsh{} Generate batch of fake labels}
                          \PY{n}{gen\PYZus{}labels} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes}\PY{p}{,}\PY{p}{(}\PY{n}{b\PYZus{}size}\PY{p}{,}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{type}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{)} \PY{c+c1}{\PYZsh{} torch.Size([128, 3])}
          
                          \PY{c+c1}{\PYZsh{} Generate fake image batch with G}
                          \PY{n}{fake} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG}\PY{p}{(}\PY{n}{noise}\PY{p}{,} \PY{n}{gen\PYZus{}labels}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Update ground truth labels to FAKE}
                          \PY{n}{validity\PYZus{}label}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fake\PYZus{}label}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Classify all fake batch with D}
                          \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{p}{(}\PY{n}{fake}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{gen\PYZus{}labels}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate D\PYZsq{}s loss on the all\PYZhy{}fake batch}
                          \PY{n}{errD\PYZus{}fake} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{validity\PYZus{}label}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate the gradients for this batch}
                          \PY{n}{errD\PYZus{}fake}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                          \PY{n}{D\PYZus{}G\PYZus{}z1} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Add the gradients from the all\PYZhy{}real and all\PYZhy{}fake batches}
                          \PY{n}{errD} \PY{o}{=} \PY{n}{errD\PYZus{}real} \PY{o}{+} \PY{n}{errD\PYZus{}fake}
          
                          \PY{c+c1}{\PYZsh{} Update D}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerD}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
                          \PY{c+c1}{\PYZsh{} (2) Update G network: maximize log(D(G(z)))}
                          \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
          
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
          
                          \PY{n}{validity\PYZus{}label}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{real\PYZus{}label}\PY{p}{)}  \PY{c+c1}{\PYZsh{} fake labels are real for generator cost}
          
                          \PY{c+c1}{\PYZsh{} Since we just updated D, perform another forward pass of all\PYZhy{}fake batch through D}
                          \PY{n}{output} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{p}{(}\PY{n}{fake}\PY{p}{,} \PY{n}{gen\PYZus{}labels}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate G\PYZsq{}s loss based on this output}
                          \PY{n}{errG} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion}\PY{p}{(}\PY{n}{output}\PY{p}{,} \PY{n}{validity\PYZus{}label}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Calculate gradients for G}
                          \PY{n}{errG}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
                          \PY{n}{D\PYZus{}G\PYZus{}z2} \PY{o}{=} \PY{n}{output}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Update G}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerG}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Output training stats}
                          \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{50} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                              \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{[}\PY{l+s+si}{\PYZob{}epoch\PYZcb{}}\PY{l+s+s1}{/}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{self.start\PYZus{}epoch + self.num\PYZus{}epochs \PYZhy{}}
          \PY{l+m+mi}{1}\PY{p}{\PYZcb{}}\PY{p}{]}\PY{p}{[}\PY{p}{\PYZob{}}\PY{n}{i}\PY{p}{\PYZcb{}}\PY{o}{/}\PY{p}{\PYZob{}}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dataloader}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{]}\PYZbs{}\PY{n}{tLoss\PYZus{}D}\PY{p}{:} \PY{p}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{errD}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{\PYZcb{}}\PYZbs{}\PY{n}{tLoss\PYZus{}G}\PY{p}{:}
          \PY{p}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{errG}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{\PYZcb{}}\PYZbs{}\PY{n}{tD}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:} \PY{p}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{D\PYZus{}x}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{\PYZcb{}}\PYZbs{}\PY{n}{tD}\PY{p}{(}\PY{n}{G}\PY{p}{(}\PY{n}{z}\PY{p}{)}\PY{p}{)}\PY{p}{:} \PY{p}{\PYZob{}}\PY{n+nb}{round}\PY{p}{(}\PY{n}{D\PYZus{}G\PYZus{}z1}\PY{o}{/}\PY{n}{D\PYZus{}G\PYZus{}z2}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{)}
          
                          \PY{c+c1}{\PYZsh{} Save Losses for plotting later}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{errG}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
                          \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{D\PYZus{}losses}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{errD}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Check how the generator is doing by saving G\PYZsq{}s output on fixed\PYZus{}noise}
                          \PY{c+c1}{\PYZsh{} every 500 iterations, or on the last batch of the last epoch}
                          \PY{k}{if} \PY{p}{(}\PY{n}{iters} \PY{o}{\PYZpc{}} \PY{l+m+mi}{500} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)} \PY{o+ow}{or} \PY{p}{(}\PY{p}{(}\PY{n}{epoch} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{num\PYZus{}epochs}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{i} \PY{o}{==}
          \PY{n+nb}{len}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dataloader}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
          
                              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Saving a batch of fake images.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
                              \PY{n}{class\PYZus{}index} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}classes}\PY{p}{)} \PY{c+c1}{\PYZsh{} get class indices}
                              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{class\PYZus{}index}\PY{p}{:}
                                  \PY{n}{class\PYZus{}index\PYZus{}tensor} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{)}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{c+c1}{\PYZsh{} repeat the class index n times}
                                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{generate\PYZus{}fake\PYZus{}images}\PY{p}{(}\PY{n}{class\PYZus{}index\PYZus{}tensor}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fixed\PYZus{}noise}\PY{p}{,}
                                                            \PY{n}{image\PYZus{}name} \PY{o}{=}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}self.classes[i]\PYZcb{}}\PY{l+s+s1}{\PYZus{}e}\PY{l+s+si}{\PYZob{}epoch\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{save} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
          
                          \PY{n}{iters} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
          
                      \PY{c+c1}{\PYZsh{} automatically save model for first epoch (testing) and every 5 epochs}
                      \PY{k}{if} \PY{n}{epoch} \PY{o}{==} \PY{l+m+mi}{1} \PY{o+ow}{or} \PY{n}{epoch} \PY{o}{\PYZpc{}} \PY{l+m+mi}{5} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
          
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Finished Training for }\PY{l+s+si}{\PYZob{}epoch\PYZcb{}}\PY{l+s+s2}{ epochs.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{epoch}\PY{p}{)}
          
          
              \PY{k}{def} \PY{n+nf}{save}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{epoch}\PY{p}{)}\PY{p}{:}
          
                  \PY{c+c1}{\PYZsh{} save the model checkpoint}
                  \PY{n}{filepath} \PY{o}{=} \PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}self.checkpoint\PYZus{}dir\PYZcb{}}\PY{l+s+s1}{/checkpoint\PYZus{}e}\PY{l+s+si}{\PYZob{}epoch\PYZcb{}}\PY{l+s+s1}{.pth.tar}\PY{l+s+s1}{\PYZsq{}}
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=\PYZgt{} Saving checkpoint: }\PY{l+s+si}{\PYZob{}filepath\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
                  \PY{n}{state} \PY{o}{=} \PY{p}{\PYZob{}}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{D\PYZus{}losses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{D\PYZus{}losses}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{G\PYZus{}losses}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}losses}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{epoch}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netD\PYZus{}state\PYZus{}dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerD}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netG\PYZus{}state\PYZus{}dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                      \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerG}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                  \PY{p}{\PYZcb{}}
          
                  \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{state}\PY{p}{,} \PY{n}{filepath}\PY{p}{)}
          
          
              \PY{k}{def} \PY{n+nf}{load}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{loadpath}\PY{p}{)}\PY{p}{:}
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        When loading model checkpoint, just load the epoch and state dicts to continue training.}
          \PY{l+s+sd}{        The D\PYZhy{}loss and G\PYZhy{}loss can be stored within their respective checkpoints}
          \PY{l+s+sd}{        and referred to later when needed.}
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
                  \PY{k}{if} \PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{isfile}\PY{p}{(}\PY{n}{loadpath}\PY{p}{)}\PY{p}{:}
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=\PYZgt{} loading checkpoint: }\PY{l+s+si}{\PYZob{}loadpath\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                      \PY{n}{checkpoint} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{loadpath}\PY{p}{)}
          
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{start\PYZus{}epoch} \PY{o}{=} \PY{n}{checkpoint}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{+} \PY{l+m+mi}{1}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netD}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{checkpoint}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netD\PYZus{}state\PYZus{}dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{netG}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{checkpoint}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netG\PYZus{}state\PYZus{}dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerD}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{checkpoint}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizerG}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{checkpoint}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=\PYZgt{} loaded checkpoint: }\PY{l+s+si}{\PYZob{}loadpath\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Last epoch was }\PY{l+s+si}{\PYZob{}checkpoint[\PYZsq{}epoch\PYZsq{}]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
                  \PY{k}{else}\PY{p}{:}
                      \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{=\PYZgt{} No checkpoint found at: }\PY{l+s+si}{\PYZob{}loadpath\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
          
              \PY{k}{def} \PY{n+nf}{visualize\PYZus{}results}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
          
                  \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Generator and Discriminator Loss During Training}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{G\PYZus{}losses}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{G}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{D\PYZus{}losses}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{D}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{iterations}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Loss}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \subsection*{CIFAR10 Model}\label{cifar10-model}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}287}]:} \PY{c+c1}{\PYZsh{} Create the Discriminator}
          \PY{n}{netD} \PY{o}{=} \PY{n}{cDiscriminator}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cifar\PYZus{}classes}\PY{p}{)}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Apply the weights\PYZus{}init function to randomly initialize all weights to mean=0,}
          \PY{n}{stdev}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{o}{.}
          \PY{n}{netD}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{weights\PYZus{}init}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the model}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{netD}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create the generator}
          \PY{n}{netG} \PY{o}{=} \PY{n}{cGenerator}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{cifar\PYZus{}classes}\PY{p}{)}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Apply the weights\PYZus{}init function to randomly initialize all weights to mean=0,}
          \PY{n}{stdev}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{o}{.}
          \PY{n}{netG}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{weights\PYZus{}init}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the model}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{netG}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
cDiscriminator(
  (label\_embedding): Embedding(10, 10)
  (convolution\_layers): Sequential(
    (0): Conv2d(3, 32, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative\_slope=0.2, inplace)
    (2): Conv2d(32, 64, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (4): LeakyReLU(negative\_slope=0.2, inplace)
    (5): Conv2d(64, 128, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (7): LeakyReLU(negative\_slope=0.2, inplace)
    (8): Conv2d(128, 1, kernel\_size=(4, 4), stride=(1, 1), bias=False)
  )
  (linear\_layers): Sequential(
    (0): Linear(in\_features=11, out\_features=512, bias=True)
    (1): LeakyReLU(negative\_slope=0.2, inplace)
    (2): Linear(in\_features=512, out\_features=1, bias=True)
    (3): Sigmoid()
  )
)
cGenerator(
  (label\_emb): Embedding(10, 10)
  (main): Sequential(
    (0): ConvTranspose2d(110, 256, kernel\_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (2): ReLU(inplace)
    (3): ConvTranspose2d(256, 128, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (5): ReLU(inplace)
    (6): ConvTranspose2d(128, 32, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (8): ReLU(inplace)
    (9): ConvTranspose2d(32, 3, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (10): Tanh()
  )
)

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}288}]:} \PY{n}{kwargs} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataloader}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{trainloader}\PY{p}{,} \PY{c+c1}{\PYZsh{} cifar dataloader}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{cifar\PYZus{}classes}\PY{p}{,} \PY{c+c1}{\PYZsh{} cifar classes}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}dir}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN\PYZhy{}cifar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{num\PYZus{}epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{40}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{criterion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{nn}\PY{o}{.}\PY{n}{BCELoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Initialize BCELoss function}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{netD}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{netG}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{netD}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{n}{beta1}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{netG}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{n}{beta1}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{device}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{device}
          \PY{p}{\PYZcb{}}
          
          \PY{n}{cdcgan\PYZus{}cifar} \PY{o}{=} \PY{n}{cDCGAN}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
cDCGAN-cifar directory exists.
cDCGAN-cifar/checkpoints directory exists.
cDCGAN-cifar/fake\_images directory exists.

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}289}]:} \PY{n}{cdcgan\PYZus{}cifar}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN\PYZhy{}cifar/checkpoints/checkpoint\PYZus{}e60.pth.tar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
=> loading checkpoint: cDCGAN-cifar/checkpoints/checkpoint\_e60.pth.tar
=> loaded checkpoint: cDCGAN-cifar/checkpoints/checkpoint\_e60.pth.tar
Last epoch was 60

    \end{Verbatim}

    \subsection*{Novel Dataset Model}\label{novel-dataset-model}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}153}]:} \PY{c+c1}{\PYZsh{} Create the Discriminator}
          \PY{n}{netD} \PY{o}{=} \PY{n}{cDiscriminator}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dataset}\PY{o}{.}\PY{n}{classes}\PY{p}{)}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Apply the weights\PYZus{}init function to randomly initialize all weights to mean=0,}
          \PY{n}{stdev}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{o}{.}
          \PY{n}{netD}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{weights\PYZus{}init}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the model}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{netD}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create the generator}
          \PY{n}{netG} \PY{o}{=} \PY{n}{cGenerator}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{dataset}\PY{o}{.}\PY{n}{classes}\PY{p}{)}\PY{p}{,} \PY{n}{ngpu}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Apply the weights\PYZus{}init function to randomly initialize all weights to mean=0,}
          \PY{n}{stdev}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{o}{.}
          \PY{n}{netG}\PY{o}{.}\PY{n}{apply}\PY{p}{(}\PY{n}{weights\PYZus{}init}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the model}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{netG}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
cDiscriminator(
  (label\_embedding): Embedding(3, 3)
  (convolution\_layers): Sequential(
    (0): Conv2d(3, 32, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative\_slope=0.2, inplace)
    (2): Conv2d(32, 64, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (4): LeakyReLU(negative\_slope=0.2, inplace)
    (5): Conv2d(64, 128, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (7): LeakyReLU(negative\_slope=0.2, inplace)
    (8): Conv2d(128, 1, kernel\_size=(4, 4), stride=(1, 1), bias=False)
  )
  (linear\_layers): Sequential(
    (0): Linear(in\_features=4, out\_features=512, bias=True)
    (1): LeakyReLU(negative\_slope=0.2, inplace)
    (2): Linear(in\_features=512, out\_features=1, bias=True)
    (3): Sigmoid()
  )
)
cGenerator(
  (label\_emb): Embedding(3, 3)
  (main): Sequential(
    (0): ConvTranspose2d(103, 256, kernel\_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (2): ReLU(inplace)
    (3): ConvTranspose2d(256, 128, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (5): ReLU(inplace)
    (6): ConvTranspose2d(128, 32, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True,
track\_running\_stats=True)
    (8): ReLU(inplace)
    (9): ConvTranspose2d(32, 3, kernel\_size=(4, 4), stride=(2, 2), padding=(1, 1),
bias=False)
    (10): Tanh()
  )
)

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}154}]:} \PY{n}{kwargs} \PY{o}{=} \PY{p}{\PYZob{}}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{dataloader}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{dataloader}\PY{p}{,} \PY{c+c1}{\PYZsh{} novel data dataloader}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{classes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{dataset}\PY{o}{.}\PY{n}{classes}\PY{p}{,} \PY{c+c1}{\PYZsh{} novel dataset classes}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{save\PYZus{}dir}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{num\PYZus{}epochs}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+m+mi}{0}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{criterion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{nn}\PY{o}{.}\PY{n}{BCELoss}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{c+c1}{\PYZsh{} Initialize BCELoss function}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{netD}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{netG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{netG}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerD}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{netD}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{n}{beta1}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizerG}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{netG}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{lr}\PY{p}{,} \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{n}{beta1}\PY{p}{,} \PY{l+m+mf}{0.999}\PY{p}{)}\PY{p}{)}\PY{p}{,}
              \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{device}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{device}
          \PY{p}{\PYZcb{}}
          
          \PY{n}{cdcgan} \PY{o}{=} \PY{n}{cDCGAN}\PY{p}{(}\PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
cDCGAN directory exists.
cDCGAN/checkpoints directory exists.
cDCGAN/fake\_images directory exists.

    \end{Verbatim}

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}155}]:} \PY{n}{cdcgan}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cDCGAN/checkpoints/checkpoint\PYZus{}e60.pth.tar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\},fontsize=\footnotesize]
=> loading checkpoint: cDCGAN/checkpoints/checkpoint\_e60.pth.tar
=> loaded checkpoint: cDCGAN/checkpoints/checkpoint\_e60.pth.tar
Last epoch was 60

    \end{Verbatim}

    \subsection*{Functions for Visualizing
Results}\label{functions-for-visualizing-results}

The following functions were used to visualize the generated images. The
output is seen in the results section above.

   \begin{Verbatim}[commandchars=\\\{\},fontsize=\scriptsize]
{\color{incolor}In [{\color{incolor}274}]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}gen\PYZus{}images}\PY{p}{(}\PY{n}{classes}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{no\PYZus{}of\PYZus{}images} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Plots the generated images for each class.}
          
          \PY{l+s+sd}{    Inputs}
          \PY{l+s+sd}{        classes (ARR)}
          \PY{l+s+sd}{            List of classes to be used as labels for generating fake images.}
          
          \PY{l+s+sd}{        model (MODEL)}
          \PY{l+s+sd}{            Pytorch model to be used for generating fake images.}
          
          \PY{l+s+sd}{        no\PYZus{}of\PYZus{}images (INT)}
          \PY{l+s+sd}{            Number of fake images to generated for each class.}
          \PY{l+s+sd}{            Default: 10.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{n}{noise} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{no\PYZus{}of\PYZus{}images}\PY{p}{,} \PY{n}{nz}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
          
              \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                  \PY{n}{images} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generate\PYZus{}fake\PYZus{}images}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{n}{no\PYZus{}of\PYZus{}images}\PY{p}{)}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n}{c}\PY{p}{)}\PY{p}{,}\PY{n}{noise} \PY{o}{=} \PY{n}{noise}\PY{p}{,} \PY{n}{save} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{n}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Fake }\PY{l+s+si}{\PYZob{}classes[c]\PYZcb{}}\PY{l+s+s1}{ (epoch = 60)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{vutils}\PY{o}{.}\PY{n}{make\PYZus{}grid}\PY{p}{(}\PY{n}{images}\PY{p}{,} \PY{n}{nrow} \PY{o}{=} \PY{n}{no\PYZus{}of\PYZus{}images}\PY{p}{,} \PY{n}{normalize}\PY{o}{=} \PY{k+kc}{True}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
          
          
          \PY{k}{def} \PY{n+nf}{plot\PYZus{}progression}\PY{p}{(}\PY{n}{my\PYZus{}dir}\PY{p}{,} \PY{n}{class\PYZus{}name}\PY{p}{,} \PY{n}{fig\PYZus{}w}\PY{p}{,} \PY{n}{fig\PYZus{}h}\PY{p}{,} \PY{n}{nrows}\PY{p}{,} \PY{n}{ncols}\PY{p}{,} \PY{n}{info} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    Plot the cDCGAN\PYZsq{}s image generation progression throughout the training process.}
          
          \PY{l+s+sd}{    Inputs}
          \PY{l+s+sd}{        my\PYZus{}dir (STR)}
          \PY{l+s+sd}{            Directory of generator progression images.}
          \PY{l+s+sd}{            Images should be labelled with digit numbers for sorting.}
          
          \PY{l+s+sd}{        class\PYZus{}name (STR)}
          \PY{l+s+sd}{            Specific class label to be plotted.}
          
          \PY{l+s+sd}{        fig\PYZus{}w (INT)}
          \PY{l+s+sd}{            Figure width.}
          
          \PY{l+s+sd}{        fig\PYZus{}h (INT)}
          \PY{l+s+sd}{            Figure height.}
          
          \PY{l+s+sd}{        nrows (INT)}
          \PY{l+s+sd}{            Number of rows in plot.}
          
          \PY{l+s+sd}{        ncols (INT)}
          \PY{l+s+sd}{            Number of columns in plot.}
          
          \PY{l+s+sd}{        info (BOOL)}
          \PY{l+s+sd}{            If true, displays the number of images to be plotted and the list of image names.}
          \PY{l+s+sd}{            Default: False.}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              \PY{n}{dir\PYZus{}list} \PY{o}{=} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{n}{my\PYZus{}dir}\PY{p}{)} \PY{c+c1}{\PYZsh{} get list of images in directory}
              \PY{n}{img\PYZus{}list} \PY{o}{=} \PY{p}{[}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{dir\PYZus{}list} \PY{k}{if} \PY{n}{class\PYZus{}name} \PY{o+ow}{in} \PY{n}{i}\PY{p}{]} \PY{c+c1}{\PYZsh{} get list of images in a specific class}
              \PY{n+nc}{img\PYZus{}list}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{f}\PY{p}{:} \PY{n+nb}{int}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{n+nb}{str}\PY{o}{.}\PY{n}{isdigit}\PY{p}{,} \PY{n}{f}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} sort in ascending order}
          
              \PY{k}{if} \PY{n}{info}\PY{p}{:}
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{There are }\PY{l+s+s2}{\PYZob{}}\PY{l+s+s2}{len(img\PYZus{}list)\PYZcb{} images to be plotted. Adjust params accordingly")}
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Images: }\PY{l+s+si}{\PYZob{}img\PYZus{}list\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
              \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{n}{fig\PYZus{}w}\PY{p}{,} \PY{n}{fig\PYZus{}h}\PY{p}{)}\PY{p}{)}
              \PY{n}{gs1} \PY{o}{=} \PY{n}{gridspec}\PY{o}{.}\PY{n}{GridSpec}\PY{p}{(}\PY{n}{nrows}\PY{p}{,} \PY{n}{ncols}\PY{p}{)}
              \PY{n}{gs1}\PY{o}{.}\PY{n}{update}\PY{p}{(}\PY{n}{wspace}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{hspace}\PY{o}{=}\PY{l+m+mf}{0.025}\PY{p}{)} \PY{c+c1}{\PYZsh{} set the spacing between axes.}
          
              \PY{c+c1}{\PYZsh{} view generator progression}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{img\PYZus{}list}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                  \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{n}{gs1}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{axis}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{off}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}aspect}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{equal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  \PY{n}{img} \PY{o}{=} \PY{n}{mpimg}\PY{o}{.}\PY{n}{imread}\PY{p}{(}\PY{n}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}my\PYZus{}dir\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}img\PYZus{}list[i]\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{img}\PY{p}{)}
          
              \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\newpage
    \section*{References}\label{references}

Brownlee, J. (2019, July 12). How to Develop a Conditional GAN (cGAN)
From Scratch. Retrieved November 25, 2019, from
https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/.\\

Desai, U. (2018, June 8). Training a Conditional DC-GAN on CIFAR-10.
Retrieved November 25, 2019, from
https://medium.com/@utk.is.here/training-a-conditional-dc-gan-on-cifar-10-fce88395d610.\\

Goodfellow, I. (2016). NIPS 2016 tutorial: Generative adversarial
networks. arXiv preprint arXiv:1701.00160. Retrieved from
https://arxiv.org/pdf/1701.00160.pdf\\

Hui, J. (2018, July 2). GAN - Unrolled GAN (how to reduce mode
collapse). Retrieved November 28, 2019, from
https://medium.com/@jonathan\_hui/gan-unrolled-gan-how-to-reduce-mode-collapse-af5f2f7b51cd.\\

Hui, J. (2019, January 9). GAN - Ways to improve GAN performance.
Retrieved November 28, 2019, from
https://towardsdatascience.com/gan-ways-to-improve-gan-performance-acf37f9f59b.\\

Inkawhich, N. (2017). DCGAN Tutorial. Retrieved November 25, 2019, from
https://pytorch.org/tutorials/beginner/dcgan\_faces\_tutorial.html.\\

Kang, H. (2017, August 22). znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN.
Retrieved November 25, 2019, from
https://github.com/znxlwm/pytorch-MNIST-CelebA-cGAN-cDCGAN.\\

Karpathy, A., \& Johnson, J. (2017). CS231n: Convolutional Neural
Networks for Visual Recognition-Transfer Learning. Retrieved November
13, 2019, from https://cs231n.github.io/transfer-learning/\#tf.\\

Linder-Noren, E. (2019). Conditional GAN Code. Retrieved from
https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\\

Metz, L., Poole, B., Pfau, D., and Sohl-Dickstein, J. (2016). Unrolled
generative adversarial networks. arXiv preprint arXiv:1611.02163 .\\

Mirza, M., \& Osindero, S. (2014). Conditional generative adversarial
nets. arXiv preprint arXiv:1411.1784. Retrieved from
https://arxiv.org/pdf/1411.1784.pdf\\

Radford, A., Metz, L., \& Chintala, S. (2015). Unsupervised
representation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434.\\

Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and
Chen, X. (2016). Improved techniques for training gans. In Advances in
Neural Information Processing Systems, pages 2226--2234.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
